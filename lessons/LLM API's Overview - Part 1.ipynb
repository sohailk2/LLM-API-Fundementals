{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56350131-6ebf-4309-a656-7bc472013c3f",
   "metadata": {},
   "source": [
    "# Interfacing with LLM's Through APIs: Subtitle [EDIT ME]\n",
    "\n",
    "* * * \n",
    "\n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Learning Objectives - WIP\n",
    "    \n",
    "* Understand the Power of the Raw LLM Models that goes behind the tools like ChatGPT and Claude\n",
    "* Figure out how to interface with them to unlock their hidden power enabling your use case\n",
    "* As a researcher how to use LLMs directly for text extraction, etc at scale\n",
    "* Background behind how LLMs are hosted and how these are seperate from the tools that ChatGPT/etc uses\n",
    "* lol i gotta get better at writing this content. this sounds so corny\n",
    "</div>\n",
    "\n",
    "### Icons Used in This Notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "ü•ä **Challenge**: Interactive excersise. We'll work through these in the workshop!<br>\n",
    "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "‚ö†Ô∏è **Warning:** Heads-up about tricky stuff or common mistakes.<br>\n",
    "üìù **Poll:** A Zoom poll to help you learn!<br>\n",
    "üé¨ **Demo**: Showing off something more advanced ‚Äì so you know what Python can be used for!<br> \n",
    "\n",
    "[NOTE: Remove icons if they're not used in the notebook]\n",
    "\n",
    "### Sections\n",
    "1. [Section Name - EDIT ME](#section1)\n",
    "2. [Section Name - EDIT ME](#section2)\n",
    "3. [Reflection: [Title of Reflection] - EDIT ME](#refl)\n",
    "4. [Demo: [Title of Demo] - EDIT ME](#demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdf513e-04ba-4025-98a3-870385f3f3a1",
   "metadata": {},
   "source": [
    "<a id='this'></a>\n",
    "\n",
    "## This Workshop\n",
    "\n",
    "Most of us have used tools like ChatGPT or Claude, but what you may not realize is that these interfaces are **not the language models themselves**. They‚Äôre polished applications built on top of large language models (LLMs), adding helpful features like memory, chat formatting, context injection, and more.\n",
    "\n",
    "In this workshop, we‚Äôll peel back the layers and explore:\n",
    "- **How LLMs are hosted and accessed through APIs**\n",
    "- **How tools like ChatGPT are built around them**\n",
    "- **How you can interface directly with LLMs via API calls**\n",
    "\n",
    "This shift from using LLMs in chat to using them as infrastructure unlocks powerful possibilities for research:\n",
    "- Extracting structured data from unstructured text  \n",
    "- Performing thematic coding across interview transcripts  \n",
    "- Generating summaries, classifications, and annotations at scale  \n",
    "\n",
    "By the end, you'll know how to go from ‚Äúask ChatGPT a question‚Äù to let's build our own GPT like system for extraction (ehh, not really i gotta figure this out)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7387d53e-a331-4af2-8cd0-52189e17c535",
   "metadata": {},
   "source": [
    "## Motivating Scenario: Coding Therapy Dialogues at Scale\n",
    "\n",
    "You're a psychology researcher analyzing hundreds of counseling sessions between clients and therapists. Your goal is to understand:\n",
    "- What types of **issues** clients bring up\n",
    "- How they **cope** with emotional challenges\n",
    "- Whether there's any language indicating **emotional risk**\n",
    "- What **techniques** therapists use in response\n",
    "\n",
    "---\n",
    "\n",
    "### Approach 1: Manual Thematic Coding (Example)\n",
    "\n",
    "Let‚Äôs look at one real example from the [`Amod/mental_health_counseling_conversations`](https://huggingface.co/datasets/Amod/mental_health_counseling_conversations) dataset.\n",
    "\n",
    "> Client (Context):\n",
    "> \"I‚Äôve been feeling overwhelmed with school lately. It‚Äôs like no matter how hard I try, I can‚Äôt catch up. I just end up crying at night.\"\n",
    ">\n",
    "> Therapist (Response):\n",
    "> \"It‚Äôs completely understandable to feel that way under so much pressure. I‚Äôm here to support you. Can we explore what‚Äôs making you feel so behind?\"\n",
    "\n",
    "From a research perspective, you might label this exchange as:\n",
    "\n",
    "- `presenting_issue`: academic stress / anxiety  \n",
    "- `coping_style`: emotion-focused  \n",
    "- `client_emotion`: sad  \n",
    "- `risk_flag`: no  \n",
    "- `counselor_technique`: reflection\n",
    "\n",
    "You can imagine doing this manually for the first 10‚Äì20 samples, but then you realize this task gets quite tedious and daunting. \n",
    "\n",
    "---\n",
    "\n",
    "### Approach 2: ChatGPT\n",
    "\n",
    "At some point, you realize: \"What is just paste this into ChatGPT and ask it to extract that structured information for me!\" \n",
    "And you try it, and it works! Voila. Seems simple enough and you get a clean response like:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"presenting_issue\": \"anxiety\",\n",
    "  \"risk_flag\": false,\n",
    "  \"client_emotion\": \"sad\",\n",
    "  \"coping_style\": \"emotion-focused\",\n",
    "  \"counselor_technique\": \"reflection\"\n",
    "}\n",
    "```\n",
    "\n",
    "But You Still Have thousands of more more samples to get through. You still have to:\n",
    "- Manually copy and paste each sample from the UI\n",
    "- Append the output to a document or spreadsheet\n",
    "- Hope ChatGPT stays consistent\n",
    "- Repeat this thousands of times\n",
    "\n",
    "\n",
    "Seems like something that would be made easy if you could just programmatically call ChatGPT or whatever the model is that it utilizes behind the scenes.\n",
    "\n",
    "### Breakthrough Approach: Interfacing directly with the Large Language Models behind GPT\n",
    "Instead of working through a web interface, what if you could:\n",
    "- Programmatically send each sample to the model  \n",
    "- Receive structured, reliable JSON responses  \n",
    "- Save everything automatically into a CSV or database  \n",
    "- Scale up from 10 samples to 10,000\n",
    "\n",
    "Using LLMs directly you can...\n",
    "\n",
    "maybe add a visual or high level diagram of what we want to do?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a89610c-76c8-4a3f-bda8-eecf5967849b",
   "metadata": {},
   "source": [
    "## Background: LLM Hosting\n",
    "Now before we get into how you can interface with these LLM's directly through the API, let's build up some context to how LLM's are even hosted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac716ff-2121-45e4-8d93-92e141040b72",
   "metadata": {},
   "source": [
    "- image goes here -\n",
    "\n",
    "describe how there are hosted Servers that host the LLM itself, then ChatGPT is a web application, that connects to those servers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e888ddff-0161-45b7-9ace-9df825d204a6",
   "metadata": {},
   "source": [
    "### LLM Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9121e02-a383-4702-9f9f-1b2bb4efd462",
   "metadata": {},
   "source": [
    "At its core, a Large Language Model is just a giant neural network:  a trained set of weights and biases. When you ask it something you‚Äôre running what‚Äôs called a forward pass or inference. You pass in some input text, and the model predicts the next most likely word (again and again, until it's done). Theoretically, if you had a computer powerful enough you could run this set of weights and biases locally on your computer. \n",
    "\n",
    "Howevver, in practice the SoTa (state of the art models) tend to be much, much larger than what you can fit on your computer. When you use ChatGPT, Claude, or Gemini, you‚Äôre not actually running the model on your laptop. Behind the scenes, you're sending a request to a Large Language Model (LLM) hosted on a powerful server that is capable of running these giant models. They can require tens or hundreds of gigabytes of memory and need specialized hardware like GPUs.\n",
    "\n",
    "### Hosted LLMs and API Servers\n",
    "Because running an LLM requires this massive amount of compute (reword this to something simpler?), companies host them on the cloud and provide access through something called an API Server.\n",
    "\n",
    "An API (Application Programming Interface) server gives your application a way to:\n",
    "- Send text to the model (request),\n",
    "- Run an inference (get predictions) remotely,\n",
    "- And receive the result back (response)\n",
    "\n",
    "So when you interact with ChatGPT, what‚Äôs really happening under the hood is something like this:\n",
    "ChatGPT UI  ‚Üí  API Call  ‚Üí  Hosted LLM  ‚Üí  Output  ‚Üí  ChatGPT UI [need to create this image]\n",
    "\n",
    "This pattern became the foundation for how LLMs are accessed at scale.\n",
    "\n",
    "### Competing Standards for LLM APIs\n",
    "As more models were released, different organizations built their own API ‚Äúspecifications‚Äù for hosting LLMs. A few major ones emerged:\n",
    "\n",
    "- OpenAI API Spec ‚Äì The most widely adopted standard, designed by OpenAI\n",
    "- Hugging Face TGI (Text Generation Inference) ‚Äì Hugging Face‚Äôs approach to model serving\n",
    "- vLLM ‚Äì A fast, GPU-efficient serving engine from UC Berkeley & partners\n",
    "\n",
    "Each of these defined:\n",
    "- What endpoints (like /generate or /chat) should exist\n",
    "- How to format the request body (prompts, parameters, etc.)\n",
    "- What the output should look like (tokens, probabilities, completions...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b14893e-e1ac-48c3-98e5-4805bb8bc553",
   "metadata": {},
   "source": [
    "### The World Converges: OpenAI's API Format\n",
    "Similar to how phones began to converge on a single charging standard (like USB-C), the AI developer ecosystem has increasingly converged on OpenAI‚Äôs API format as a common interface for LLMs. Today, even third-party models like Mistral, Meta‚Äôs LLaMA, and others are often hosted behind APIs that support OpenAI-style endpoints, such as:\n",
    "\n",
    "- [Responses](https://platform.openai.com/docs/api-reference/responses)\n",
    "- [Chat](https://platform.openai.com/docs/api-reference/chat)\n",
    "- [Embeddings](https://platform.openai.com/docs/api-reference/embeddings)\n",
    "- Mention MCP? probably out of scope\n",
    "\n",
    "Why the convergence?\n",
    "- Developer tools (e.g., LangChain, LlamaIndex, OpenRouter) are already built around these formats\n",
    "- Developers don‚Äôt need to learn a new interface for every model (easy model swapping)\n",
    "- It became a de facto \"dialect\" for talking to LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22314ca6-1769-43ad-b7f3-ab6ad9f06211",
   "metadata": {},
   "source": [
    "# Using OpenRouter for LLM Access\n",
    "\n",
    "For the rest of this workshop, we‚Äôll be using a service called **OpenRouter** to interact with LLMs.\n",
    "\n",
    "---\n",
    "\n",
    "## What is OpenRouter?\n",
    "\n",
    "OpenRouter is a unified API gateway for Large Language Models. It allows you to send a prompt to many different models (like GPT-4, Claude, Mistral, and others) using a single, consistent interface. Though OpenRouter doesn't host the LLM servers themselves, they allow you to interact with LLM hosts through one unified API. \n",
    "\n",
    "In addition to providing one unified experience, we are using OpenRouter since it doesn't require a credit card to interact with the free tier of models (though you are limited to 50 calls per day).\n",
    "\n",
    "-- add more details about openrouter -- \n",
    "---\n",
    "\n",
    "## Getting Set Up\n",
    "\n",
    "To use OpenRouter, you‚Äôll need to:\n",
    "1. Create a free account at [https://openrouter.ai](https://openrouter.ai)\n",
    "2. Click on your profile (top right) and generate an API key\n",
    "    - Settings -> API Keys -> Create API Key\n",
    "4. ‚ö†Ô∏è **Warning:** Save this API Key Somehwere Safe\n",
    "    -  It is generally best practice to save this key somewhere secure like an environment variable or in a secure file on your system.<br>\n",
    "    - This is a very sensitive key which allows you to interface directly with the API's. Do not share it with anyone or upload it to any public accounts like Github.\n",
    "    - You will only be able to view this key once, before you can't see it again. If you lose it, you will have to create a new one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73831f44-76dd-4314-865b-87ff6fb87678",
   "metadata": {},
   "source": [
    "For the purposes of this workshop, let's make a new file in your environment called API_KEY, save it there, and then read our key directly from that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "200b4ddb-3fb1-4505-ae90-e24b4f54ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load in the API_KEY which we've saved to a file\n",
    "with open('API_KEY.txt', 'r') as file:\n",
    "    API_KEY = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c6c90d0-c6b6-43b8-82f6-2ba3771ee1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the OpenAI Python Client\n",
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2af2c4b3-b0a4-41b4-a98a-a01703408965",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the OpenAI Python Client\n",
    "from openai import OpenAI\n",
    "\n",
    "# Notice how we can utilize the OpenAI Client with non OpenAI Hosts/Models due to the unified standard\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\", \n",
    "  api_key= API_KEY # Place API Key Here,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e6d002-0606-4375-b77b-ef81df580efb",
   "metadata": {},
   "source": [
    "So now we have our client setup. Next we need to figure out which model to use. \n",
    "On OpenRouter, you can navigate to [models](https://openrouter.ai/models) tab and search free to view all free models. Notice popular models like Llama 3.2, Google Gemma, and DeepSeek V3. We will choose DeepSeek v3 as it has the largest token set, but feel free to try any other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6799643f-4f68-4f42-9c92-7c7968259362",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning:** Be careful not to run these cells too many times though. We are only allowed **50 free calls per day** with the free tier on Open Router. Any more than that, and you will be prevented from making any more requests till the next day. \n",
    "\n",
    "üí° **Tip**: You can check how many requests you've used per day by navigating to the [Activity Menu](https://openrouter.ai/activity) in OpenRouter (Top Left Menu -> Activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba134d99-cfa5-41d9-ae64-fa76b668f42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request a Completion (Output) from the Chat Completions Endpoint\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"deepseek/deepseek-chat-v3-0324:free\",\n",
    "    max_tokens=150,\n",
    "    messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"Tell me about the D-Lab at UC Berkeley in 50 words or less.\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4077e754-1a53-4b70-8725-968d2bfcbb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choices': [{'finish_reason': 'length',\n",
      "              'index': 0,\n",
      "              'logprobs': None,\n",
      "              'message': {'annotations': None,\n",
      "                          'audio': None,\n",
      "                          'content': \"UC Berkeley's **D-Lab** (Development, \"\n",
      "                                     'Data, and Design Lab) provides '\n",
      "                                     'interdisciplinary training in data '\n",
      "                                     'science, research methods, and digital '\n",
      "                                     'tools to empower students and '\n",
      "                                     'researchers. Offering workshops, '\n",
      "                                     'consulting, and courses, D-Lab focuses '\n",
      "                                     'on practical skills for impactful, '\n",
      "                                     'data-driven work across social sciences, '\n",
      "                                     'humanities',\n",
      "                          'function_call': None,\n",
      "                          'reasoning': None,\n",
      "                          'refusal': None,\n",
      "                          'role': 'assistant',\n",
      "                          'tool_calls': None},\n",
      "              'native_finish_reason': 'length'}],\n",
      " 'created': 1753223160,\n",
      " 'id': 'gen-1753223155-9DndxtRa1GyCHdoX4FNY',\n",
      " 'model': 'deepseek/deepseek-chat-v3-0324:free',\n",
      " 'object': 'chat.completion',\n",
      " 'provider': 'AtlasCloud',\n",
      " 'service_tier': None,\n",
      " 'system_fingerprint': None,\n",
      " 'usage': {'completion_tokens': 60,\n",
      "           'completion_tokens_details': None,\n",
      "           'prompt_tokens': 21,\n",
      "           'prompt_tokens_details': None,\n",
      "           'total_tokens': 81}}\n"
     ]
    }
   ],
   "source": [
    "# Pretty Printing The Completion Response\n",
    "from pprint import pprint\n",
    "pprint(completion.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce169e4-224a-4811-99f1-6cfc1a107954",
   "metadata": {},
   "source": [
    "Notice the abundance of data contained in the raw Completions Object. This Object represents the response from the LLM featuring the outpue message, content, finish_reason, and a variety of other useful metadata. To get the generated response, we want to focus on the choices object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07536fec-0268-48e7-a07a-35e324f6bb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UC Berkeley's **D-Lab** (Development, Data, and Design Lab) provides interdisciplinary training in data science, research methods, and digital tools to empower students and researchers. Offering workshops, consulting, and courses, D-Lab focuses on practical skills for impactful, data-driven work across social sciences, humanities\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af96f9ef-c90e-49bd-9436-9b40c0b3515a",
   "metadata": {},
   "source": [
    "Notice the broad knowledge base of the DeepSeek Model which contains information about UC Berkley's very own D-Lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c812746-d3a0-4852-a6e7-950d199770b6",
   "metadata": {},
   "source": [
    "### Congrats! This marks the end of the first part of the workshop! \n",
    "\n",
    "In Part 2, we will break down the parameters that go into constructing this call what it all means. And how you can use this to build your own scalable workflows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
